import re

def parse_agent_response(response):
    """ç»“æ„åŒ–è§£æAPIå“åº”"""
    result = {
        "final_answer": "",
        "execution_steps": [],
        "search_links": []
    }
    
    current_step = {}
    link_pattern = re.compile(r'https?://\S+')
    # æ–°å¢è¯¦ç»†ç»“æœåŒ¹é…æ¨¡å¼
    detail_pattern = re.compile(r'### 2\. Task outcome \(extremely detailed version\):(.+?)(?=### 3|\Z)', re.DOTALL)

    for msg in response:
        content = str(msg.get('content', ''))
        metadata = msg.get('metadata', {})
        
        # è§£ææœ€ç»ˆç­”æ¡ˆ
        if "Final answer:" in content:
            result["final_answer"] = _extract_final_answer(content)
        
        # è§£ææ‰§è¡Œæ­¥éª¤ï¼ˆä»…å¤„ç†assistantæ¶ˆæ¯ï¼‰
        if msg.get('role') == 'assistant' and content.startswith("**Step"):
            current_step = {
                "title": content.strip('* '),
                "details": []
            }
            result["execution_steps"].append(current_step)
        elif current_step and msg.get('role') == 'assistant':
            # ç²¾ç¡®æå–è¯¦ç»†åˆ†æå†…å®¹
            if (detail_match := detail_pattern.search(content)):
                cleaned_content = re.sub(r'\*{2,}|`{3,}', '',
                    detail_match.group(1)).strip()
                current_step["details"].append(cleaned_content)
        
        # æå–æ‰§è¡Œæ—¥å¿—ä¸­çš„é“¾æ¥
        if metadata.get('title') == 'ğŸ“ Execution Logs':
            result["search_links"].extend(
                link_pattern.findall(content)
            )

    return result

def _extract_final_answer(content):
    """æå–å¹¶æ¸…ç†æœ€ç»ˆç­”æ¡ˆï¼Œå°†å­—å…¸æ ¼å¼è½¬æ¢ä¸ºMarkdown"""
    # æå–Final answeråçš„å†…å®¹
    answer = content.split("Final answer:")[-1].strip()
    # ç§»é™¤æ˜Ÿå·ç­‰æ ‡è®°
    answer = re.sub(r'\*{2,}', '', answer).strip()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå­—å…¸æ ¼å¼
    dict_match = re.search(r'\{.*\}', answer, re.DOTALL)
    if dict_match:
        try:
            # å°è¯•æå–novelty_scoreå’Œreport
            score_match = re.search(r"'novelty_score':\s*(\d+)", answer)
            report_match = re.search(r"'report':\s*'(.*?)'(?=\}$|\}\s*$)", answer, re.DOTALL)
            
            if score_match and report_match:
                # æå–åˆ†æ•°
                score = f"# Novelty Score: {score_match.group(1)}/100"
                
                # æå–å¹¶æ¸…ç†æŠ¥å‘Šå†…å®¹
                report = report_match.group(1)
                # å¤„ç†è½¬ä¹‰å­—ç¬¦
                report = report.replace('\\n', '\n').replace("\\'", "'").replace('\\"', '"')
                
                # è¿”å›æ ¼å¼åŒ–çš„Markdown
                return f"{score}\n\n{report}"
        except Exception:
            pass
    
    # å¦‚æœä¸æ˜¯å­—å…¸æ ¼å¼æˆ–è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹
    return answer

def save_as_markdown(result, filename):
    """ç”Ÿæˆä¼˜åŒ–åçš„MarkdownæŠ¥å‘Š"""
    with open(filename, 'w', encoding='utf-8') as f:
        # æœ€ç»ˆç­”æ¡ˆéƒ¨åˆ†
        f.write("## MoonshotAI: Your Startup Novelty Deep Research Report\n")
        f.write(result["final_answer"] + "\n")
        
        # ç ”ç©¶æ­¥éª¤éƒ¨åˆ† - æ·»åŠ æ­¥éª¤å»é‡
        if result["execution_steps"]:
            f.write("\n### Execution Steps\n")
            seen_steps = set()
            unique_steps = []
            
            for step in result["execution_steps"]:
                # ä½¿ç”¨æ ‡é¢˜å’Œè¯¦æƒ…å†…å®¹ä½œä¸ºå”¯ä¸€æ€§åˆ¤æ–­
                step_content = (step['title'], tuple(step['details']))
                if step_content not in seen_steps:
                    seen_steps.add(step_content)
                    unique_steps.append(step)
            
            # è¾“å‡ºå»é‡åçš„æ­¥éª¤
            for step in unique_steps:
                f.write(f"\n#### {step['title']}\n")
                f.write('\n'.join(step["details"]) + "\n")
        
        # æœç´¢ç»“æœéƒ¨åˆ†
        if result["search_links"]:
            f.write("\n### Relevant References\n")
            seen = set()
            for raw_link in result["search_links"]:
                # æ›´å¼ºå¤§çš„é“¾æ¥æ¸…ç†é€»è¾‘
                # 1. ç§»é™¤URLå‚æ•°
                link = raw_link.split('?')[0]
                # 2. å¤„ç†é“¾æ¥åçš„é¢å¤–å†…å®¹ï¼ˆåŒ…æ‹¬æ‹¬å·å’Œæ¢è¡Œç¬¦ï¼‰
                link = re.sub(r'\).*$', '', link)
                # 3. æ¸…ç†å„ç§ç‰¹æ®Šå­—ç¬¦
                link = re.sub(r'[\\n\s.]+$', '', link)
                # 4. å¤„ç†é“¾æ¥ä¸­çš„Markdownæ ¼å¼
                link = re.sub(r'\]\(.*$', '', link)
                
                if link and link.startswith('http') and link not in seen:
                    seen.add(link)
                    f.write(f"- {link}\n")
            
            # ç§»é™¤é‡å¤é“¾æ¥è¾“å‡º
            # f.write('\n'.join([f"- {link}" for link in result["search_links"]]))

if __name__ == "__main__":
    import json
    import os
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ„å»ºæ–‡ä»¶è·¯å¾„
    json_path = os.path.join(current_dir, "last_result.json")
    report_path = os.path.join(current_dir, "analysis_report.md")
    
    # ä»æœ¬åœ°JSONæ–‡ä»¶åŠ è½½æ•°æ®
    with open(json_path, encoding="utf-8") as f:
        test_data = json.load(f)
    
    parsed_result = parse_agent_response(test_data)
    save_as_markdown(
        parsed_result,
        report_path
    )
    print(f"å®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_path}")