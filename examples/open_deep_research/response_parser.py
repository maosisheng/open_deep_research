import re

def parse_agent_response(response):
    """ç»“æ„åŒ–è§£æAPIå“åº”"""
    result = {
        "final_answer": "",
        "execution_steps": [],
        "search_links": []
    }
    
    current_step = {}
    link_pattern = re.compile(r'https?://\S+')
    # æ–°å¢è¯¦ç»†ç»“æœåŒ¹é…æ¨¡å¼
    detail_pattern = re.compile(r'### 2\. Task outcome \(extremely detailed version\):(.+?)(?=### 3|\Z)', re.DOTALL)

    for msg in response:
        content = str(msg.get('content', ''))
        metadata = msg.get('metadata', {})
        
        # è§£ææœ€ç»ˆç­”æ¡ˆ
        if "Final answer:" in content:
            result["final_answer"] = _extract_final_answer(content)
        
        # è§£ææ‰§è¡Œæ­¥éª¤ï¼ˆä»…å¤„ç†assistantæ¶ˆæ¯ï¼‰
        if msg.get('role') == 'assistant' and content.startswith("**Step"):
            current_step = {
                "title": content.strip('* '),
                "details": []
            }
            result["execution_steps"].append(current_step)
        elif current_step and msg.get('role') == 'assistant':
            # ç²¾ç¡®æå–è¯¦ç»†åˆ†æå†…å®¹
            if (detail_match := detail_pattern.search(content)):
                cleaned_content = re.sub(r'\*{2,}|`{3,}', '',
                    detail_match.group(1)).strip()
                current_step["details"].append(cleaned_content)
        
        # æå–æ‰§è¡Œæ—¥å¿—ä¸­çš„é“¾æ¥
        if metadata.get('title') == 'ğŸ“ Execution Logs':
            result["search_links"].extend(
                link_pattern.findall(content)
            )

    return result

def _extract_final_answer(content):
    """æå–å¹¶æ¸…ç†æœ€ç»ˆç­”æ¡ˆ"""
    answer = content.split("Final answer:")[-1]
    return re.sub(r'\*{2,}', '', answer).strip()

def save_as_markdown(result, filename):
    """ç”Ÿæˆä¼˜åŒ–åçš„MarkdownæŠ¥å‘Š"""
    with open(filename, 'w', encoding='utf-8') as f:
        # æœ€ç»ˆç­”æ¡ˆéƒ¨åˆ†
        f.write("## Novelty Research Report\n")
        f.write(result["final_answer"] + "\n")
        
        # ç ”ç©¶æ­¥éª¤éƒ¨åˆ† - æ·»åŠ æ­¥éª¤å»é‡
        if result["execution_steps"]:
            f.write("\n### Execution Steps\n")
            seen_steps = set()
            unique_steps = []
            
            for step in result["execution_steps"]:
                # ä½¿ç”¨æ ‡é¢˜å’Œè¯¦æƒ…å†…å®¹ä½œä¸ºå”¯ä¸€æ€§åˆ¤æ–­
                step_content = (step['title'], tuple(step['details']))
                if step_content not in seen_steps:
                    seen_steps.add(step_content)
                    unique_steps.append(step)
            
            # è¾“å‡ºå»é‡åçš„æ­¥éª¤
            for step in unique_steps:
                f.write(f"\n#### {step['title']}\n")
                f.write('\n'.join(step["details"]) + "\n")
        
        # æœç´¢ç»“æœéƒ¨åˆ†
        if result["search_links"]:
            f.write("\n### Relevant References\n")
            seen = set()
            for raw_link in result["search_links"]:
                # åˆ†æ­¥éª¤æ¸…ç†é“¾æ¥
                link = raw_link.split('?')[0]  # ç§»é™¤URLå‚æ•°
                link = re.sub(r'\).*', '', link)  # åˆ é™¤ç¬¬ä¸€ä¸ª)åŠå…¶åæ‰€æœ‰å†…å®¹
                link = link.strip('\\n. ')  # æ¸…ç†é¦–å°¾ç‰¹æ®Šç¬¦å·
                
                if link and link.startswith('http') and link not in seen:
                    seen.add(link)
                    f.write(f"- {link}\n")
            f.write('\n'.join([f"- {link}" for link in result["search_links"]]))

if __name__ == "__main__":
    import json
    # ä»æœ¬åœ°JSONæ–‡ä»¶åŠ è½½æ•°æ®
    with open(r"D:\007.Projects\008.deep_research\smolagents\examples\open_deep_research\last_result.json", 
             encoding="utf-8") as f:
        test_data = json.load(f)
    
    parsed_result = parse_agent_response(test_data)
    report_path = r"D:\007.Projects\008.deep_research\smolagents\examples\open_deep_research\analysis_report.md"  # æ·»åŠ è·¯å¾„å®šä¹‰
    save_as_markdown(
        parsed_result,
        report_path
    )
    print(f"å®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_path}")